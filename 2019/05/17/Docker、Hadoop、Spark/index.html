<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  <title>Docker+Hadoop+Spark简单环境搭建 | 灯泉星</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <link href="//at.alicdn.com/t/font_620856_pl6z7sid89qkt9.css" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="header-banner">
    <h1 id="title-wrap">
      <a href="/" id="title">灯泉星</a>
    </h1>
    <p id="slogan-wrap">
      <a href="/" id="slogan">Suffering is good for you.</a>
    </p>
  </div>
  <div id="header-social">
    <ul class="list-inline text-center">
      
      <li>
          <a target="_blank" href="https://www.zhihu.com/people/">
                          <span class="fa-stack fa-lg">
                                <i class="iconfont icon-zhihu"></i>
                          </span>
          </a>
      </li>
      
      
      
      <li>
          <a target="_blank" href="https://github.com/">
                          <span class="fa-stack fa-lg">
                              <i class="iconfont icon-github"></i>
                          </span>
          </a>
      </li>
      
    </ul>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Docker、Hadoop、Spark" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/05/17/Docker、Hadoop、Spark/" class="article-date">
  <time datetime="2019-05-17T06:51:22.000Z" itemprop="datePublished">2019-05-17</time>
</a>
  </div>
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Docker+Hadoop+Spark简单环境搭建
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h2><ol>
<li><p>Fedora安装Docker</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo dnf -y install dnf-plugins-core</span><br><span class="line">sudo dnf config-manager --add-repo https://download.docker.com/linux/fedora/docker-ce.repo</span><br><span class="line">sudo dnf install docker-ce docker-ce-cli containerd.io</span><br></pre></td></tr></table></figure>
</li>
<li><p>启动Docker服务：<code>service docker start</code></p>
</li>
<li><p><a href="https://hub.docker.com/r/nusbigdatacs4225/ubuntu-with-hadoop-spark" target="_blank" rel="noopener">新国大Docker镜像</a>，配置为：</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">- ubuntu</span><br><span class="line">- jdk 1.8.0_191 (/usr/java)</span><br><span class="line">- Hadoop 2.8.5 (/usr/local/hadoop)</span><br><span class="line">- Spark 2.2.0 (/usr/local/spark)</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建containers</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker run -it -h master --name master nusbigdatacs4225/ubuntu-with-hadoop-spark</span><br><span class="line">docker run -it -h slave01 --name slave01 nusbigdatacs4225/ubuntu-with-hadoop-spark</span><br><span class="line">docker run -it -h slave02 --name slave02 nusbigdatacs4225/ubuntu-with-hadoop-spark</span><br></pre></td></tr></table></figure>
</li>
<li><p>退出container：<code>exit</code></p>
</li>
<li>查看containers：<code>sudo docker ps [-a]</code></li>
<li>重启container：<code>sudo docker container start [name]</code></li>
<li>进入container命令行：<code>sudo docker attach [name]</code></li>
</ol>
<h2 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h2><ol>
<li><p>查看三个容器各自的ip，如：</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">master: 172.17.0.2</span><br><span class="line">slave01:172.17.0.3</span><br><span class="line">slave02:172.17.0.4</span><br></pre></td></tr></table></figure>
</li>
<li><p>将对应ip填入<code>/etc/hosts</code>配置中：</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/hosts</span><br><span class="line"><span class="comment"># 在文件最后增加</span></span><br><span class="line">172.17.0.2    master</span><br><span class="line">172.17.0.3    slave01</span><br><span class="line">172.17.0.4    slave02</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>vi /usr/local/hadoop/etc/hadoop/slaves</code>，增加<code>slave01 slave02</code></p>
</li>
<li><p>初始化hdfs并且运行</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/hadoop</span><br><span class="line">bin/hdfs namenode-format</span><br><span class="line">sbin/start-all.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>hdfs命令：<code>/usr/local/hadoop/bin/hdfs dfs -[命令] [参数]</code></p>
</li>
<li>停止hdfs：<code>/usr/local/hadoop/sbin/stop-all.sh</code></li>
<li>查看运行：<code>jps</code></li>
</ol>
<h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><ol>
<li><p><code>vi /usr/local/spark/conf/spark-env.sh</code>，配置hadoop与Java路径：</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/java/jdk1.8.0_191</span><br><span class="line"><span class="built_in">export</span> HADOOP_HOME=/usr/<span class="built_in">local</span>/hadoop-2.8.5</span><br><span class="line"><span class="built_in">export</span> HADOOP_CONF_DIR=/usr/<span class="built_in">local</span>/hadoop-2.8.5/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> SPARK_MASTER_IP=172.17.0.2</span><br><span class="line"><span class="built_in">export</span> SPARK_DIST_CLASSPATH=$(/usr/<span class="built_in">local</span>/hadoop/bin/hadoop classpath)</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>vi /usr/local/spark/conf/slaves</code>，配置从节点<code>localhost slave01 slave02</code></p>
</li>
</ol>
<h2 id="MapReduce-wordcount"><a href="#MapReduce-wordcount" class="headerlink" title="MapReduce wordcount"></a>MapReduce wordcount</h2><ol>
<li>创建用户目录：<code>/usr/local/hadoop/bin/hdfs dfs -mkdir input /user/</code></li>
<li>上传input(自行在其中增加需要进行wordcount的文件)：<code>/usr/local/hadoop/bin/hdfs dfs -put input /user/</code></li>
<li>运行wordcount示例：<code>/usr/local/hadoop/bin/hadoop jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.8.5.jar wordcount /user/input /user/output</code></li>
</ol>
<h2 id="Spark-wordcount"><a href="#Spark-wordcount" class="headerlink" title="Spark wordcount"></a>Spark wordcount</h2><ol>
<li>要求hdfs已启动，并将input文件上传</li>
<li><p>在master中安装pyspark：</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">apt-get update</span><br><span class="line">apt-get install python3-pip</span><br><span class="line">pip3 install pyspark</span><br></pre></td></tr></table></figure>
</li>
<li><p>创建一个python3的软链接pytho：</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/bin</span><br><span class="line">ln -s python3 python</span><br></pre></td></tr></table></figure>
</li>
<li><p>python实现wordcount</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># wordcount.py</span></span><br><span class="line">from pyspark import SparkContext</span><br><span class="line">from time import time</span><br><span class="line"></span><br><span class="line">start = time()</span><br><span class="line">sc = SparkContext(<span class="string">'local'</span>, <span class="string">'wordcount'</span>)</span><br><span class="line">text_file = sc.textFile(<span class="string">"hdfs://master:9000/user/input"</span>)</span><br><span class="line">counts = text_file.flatMap(lambda line: line.split(<span class="string">" "</span>)) \</span><br><span class="line">            .map(lambda word: (word, 1)) \</span><br><span class="line">            .reduceByKey(lambda a, b: a + b)</span><br><span class="line">counts.saveAsTextFile(<span class="string">"hdfs://master:9000/user/output"</span>)</span><br><span class="line">elapsed = (time() - start)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Time used:"</span>, int(elapsed * 1000))</span><br></pre></td></tr></table></figure>
</li>
<li><p>运行</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python wordcount.py</span><br></pre></td></tr></table></figure>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      
    </footer>
  </div>
</article>

</section>
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2019 Quanxin Deng<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
  </div>
</body>
</html>